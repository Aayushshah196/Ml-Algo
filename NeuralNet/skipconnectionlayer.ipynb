{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConn(Layer):\n",
    "    def __init__(self, layer1, layer2, input_size, output_size, activation=\"ReLU\"):\n",
    "        self.__super__()\n",
    "        self.weight1 = np.random.rand(layer1.get_input_size(), output_size) - 0.5\n",
    "        self.weight2 = np.random.rand(layer2.get_input_size(), output_size) - 0.5\n",
    "        self.bias1 = np.random.rand(1, output_size) - 0.5\n",
    "        self.bias2 = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, A_prev1, A_prev2):\n",
    "        \"\"\"\n",
    "        A_prev1 : Input to the layer that is output of previous layer\n",
    "        A_prev2 : Input to the layer that is skip connection of past layer\n",
    "        \"\"\"\n",
    "        self.linear_cache = (A_prev1, A_prev2)\n",
    "        \n",
    "        self.activation_cache = (np.dot(A_prev1, self.weight1) + self.bias1, np.dot(A_prev2, self.weight2)+self.bias2)\n",
    "\n",
    "        if self.activation == \"ReLU\":\n",
    "            Z = (self._relu(self.activation_cache[0]), self._relu(self.activation[1]))\n",
    "\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            Z = (self._sigmoid(self.activation_cache[0]), self._sigmoid(self.activation[1]))\n",
    "\n",
    "        elif self.activation == \"tanh\":\n",
    "            Z = (self._tanh(self.activation_cache[0]), self._tanh(self.activation[1]))\n",
    "\n",
    "        assert Z[0].shape == Z[1].shape\n",
    "\n",
    "        return Z[0] + Z[1]\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward(self, dA, learning_rate):\n",
    "        dZ = None\n",
    "        if self.activation == \"ReLU\":\n",
    "            dZ = (self._relu_backward(self.activation_cache[0]) * dA, self._relu_backward(self.activation_cache[1]))\n",
    "\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = (self._sigmoid_backward(self.activation_cache[0]) * dA, self._sigmoid_backward(self.activation_cache[1]))\n",
    "        \n",
    "        elif self.activation == \"tanh\":\n",
    "            dZ = (self._tanh_backward(self.activation_cache[0]) * dA, self._tanh_backward(self.activation_cache[1]))\n",
    "        \n",
    "\n",
    "        dW1 = np.dot((self.linear_cache[0]).T, dZ[0])\n",
    "        dW2 = np.dot((self.linear_cache[1]).T, dZ[1])\n",
    "\n",
    "        # dBias = output_error\n",
    "        dB1 = np.sum(dZ[0], axis=0, keepdims=True) / dZ[0].shape[0]\n",
    "        dB2 = np.sum(dZ[1], axis=0, keepdims=True) / dZ[1].shape[0]\n",
    "\n",
    "        dA1 = np.dot(dA[0], self.weight[0].T)        \n",
    "        dA2 = np.dot(dA[1], self.weight[1].T)\n",
    "\n",
    "        # update parameters\n",
    "        self.weight1 -= learning_rate * dW1\n",
    "        self.bias1 -= learning_rate * dB1\n",
    "\n",
    "        self.weight2 -= learning_rate * dW2\n",
    "        self.bias2 -= learning_rate * dB2\n",
    "\n",
    "        return dA1, dA2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation=\"ReLU\"):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return self.input_size\n",
    "\n",
    "    def get_output_size(self):\n",
    "        return self.output_size\n",
    "\n",
    "    def _relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def _sigmoid(self, Z):\n",
    "        return 1/(1 + np.exp(-Z))\n",
    "\n",
    "    def _pseudosigmoid(self, Z):\n",
    "        return 1/(1 + np.exp(Z))\n",
    "\n",
    "    def _tanh(self, Z):\n",
    "      return np.tanh(Z)\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _relu_backward(self, activation_cache):\n",
    "        return np.where(activation_cache > 0, 1, 0)\n",
    "\n",
    "    def _sigmoid_backward(self, activation_cache):\n",
    "        _sig = self._sigmoid(activation_cache)\n",
    "        return (_sig * (1 - _sig))\n",
    "    \n",
    "    def _pseudosigmoid_backward(self, activation_cache):\n",
    "        _sig = self._pseudosigmoid(activation_cache)\n",
    "        return (_sig * (_sig - 1))\n",
    "\n",
    "    def _tanh_backward(self, activation_cache):\n",
    "        # _sig = self._tanh(self.activation_cache)\n",
    "        return ((1-np.tanh(activation_cache)**2))\n",
    "\n",
    "    def backward(self, dA, learning_rate):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size, activation=\"ReLU\"):\n",
    "        Layer.__init__(self, input_size, output_size, activation)\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.activation = activation\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"--------------------------------------------------------------\")\n",
    "        print(f\"Weights : {self.weights.shape} \\t bias : {self.bias.shape}\")\n",
    "        print(f\"Input Shape : (None, {self.input_size})\\nOutput Shape : (None, {self.output_size})\")\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, A_prev):\n",
    "        self.linear_cache = A_prev\n",
    "        self.activation_cache = np.dot(self.linear_cache, self.weights) + self.bias\n",
    "\n",
    "        if self.activation == \"ReLU\":\n",
    "            Z = self._relu(self.activation_cache)\n",
    "\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            Z = self._sigmoid(self.activation_cache)\n",
    "\n",
    "        elif self.activation == \"pseudosigmoid\":\n",
    "            Z = self._pseudosigmoid(self.activation_cache)\n",
    "\n",
    "        elif self.activation == \"tanh\":\n",
    "            Z = self._tanh(self.activation_cache)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward(self, dA, learning_rate):\n",
    "        dZ = None\n",
    "        if self.activation == \"ReLU\":\n",
    "            dZ = self._relu_backward(self.activation_cache) * dA\n",
    "        \n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = self._sigmoid_backward(self.activation_cache) * dA\n",
    "\n",
    "        elif self.activation == \"pseudosigmoid\":\n",
    "            dZ = self._pseudosigmoid_backward(self.activation_cache) * dA\n",
    "\n",
    "        elif self.activation == \"tanh\":\n",
    "            dZ = self._tanh_backward(self.activation_cache) * dA\n",
    "        \n",
    "        dW = np.dot(self.linear_cache.T, dZ)\n",
    "        # dBias = output_error\n",
    "        dB = np.sum(dZ, axis=0, keepdims=True) / dZ.shape[0]\n",
    "\n",
    "        dA = np.dot(dA, self.weights.T)\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * dB\n",
    "\n",
    "        return dA\n",
    "\n",
    "class SkipConn(Layer):\n",
    "    def __init__(self, layer1, layer2, input_size, output_size, activation=\"ReLU\"):\n",
    "        self.__super__()\n",
    "        self.weight1 = np.random.rand(layer1.get_input_size(), output_size) - 0.5\n",
    "        self.weight2 = np.random.rand(layer2.get_input_size(), output_size) - 0.5\n",
    "        self.bias1 = np.random.rand(1, output_size) - 0.5\n",
    "        self.bias2 = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, A_prev1, A_prev2):\n",
    "        \"\"\"\n",
    "        A_prev1 : Input to the layer that is output of previous layer\n",
    "        A_prev2 : Input to the layer that is skip connection of past layer\n",
    "        \"\"\"\n",
    "        self.linear_cache = (A_prev1, A_prev2)\n",
    "        \n",
    "        temp_A_prev2 = np.zeros_like(A_prev1)\n",
    "        shape = A_prev1.shape[1] if A_prev1.shape[1]<A_prev2.shape[1] else A_prev2.shape[1]\n",
    "        temp_A_prev2[:,:shape] = temp_A_prev2[:,:shape] + A_prev2[:,:shape]\n",
    "        \n",
    "        assert A_prev1.shape == temp_A_prev2.shape\n",
    "        \n",
    "        self.activation_cache = np.dot(A_prev1+temp_A_prev2, self.weights) + self.bias\n",
    "\n",
    "        if self.activation == \"ReLU\":\n",
    "            Z = self._relu(self.activation_cache)\n",
    "\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            Z = self._sigmoid(self.activation_cache)\n",
    "\n",
    "        elif self.activation == \"tanh\":\n",
    "            Z = self._tanh(self.activation_cache)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward(self, dA, learning_rate):\n",
    "        dZ = None\n",
    "        if self.activation == \"ReLU\":\n",
    "            dZ = self._relu_backward() * dA\n",
    "        \n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = self._sigmoid_backward() * dA\n",
    "        \n",
    "        elif self.activation == \"tanh\":\n",
    "            dZ = self._tanh_backward() * dA\n",
    "\n",
    "        temp_A_prev2 = np.zeros_like(self.linear_cache[0])\n",
    "        shape = self.linear_cache[0].shape[1] if self.linear_cache[0].shape[1]<self.linear_cache[1].shape[1] else self.linear_cache[1].shape[1]\n",
    "        temp_A_prev2[:,:shape] = temp_A_prev2[:,:shape] + self.linear_cache[1][:,:shape]\n",
    "        \n",
    "        \n",
    "        dW = np.dot((self.linear_cache[0]+temp_A_prev2).T, dZ)\n",
    "        # dBias = output_error\n",
    "        dB = np.sum(dZ, axis=0, keepdims=True) / dZ.shape[0]\n",
    "\n",
    "        dA = np.dot(dA, self.weights.T)\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * dB\n",
    "\n",
    "        return dA\n",
    "\n",
    "\n",
    "def batch_iterator(X, y, batch_size=2):\n",
    "\tn_samples = X.shape[1]\n",
    "\tX, y = shuffle_data(X, y)\n",
    "\tfor i in np.arange(0, n_samples, batch_size):\n",
    "\t\tbegin, end = i, min(i+batch_size-1, n_samples)\n",
    "\n",
    "\t\tyield X[begin:end,:], y[begin:end,:]\n",
    "\n",
    "def shuffle_data(X, y, seed=None):\n",
    "\tif not seed:\n",
    "\t\tseed = X.shape[0]\n",
    "\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\t# idx  = np.arange(X.shape[0])\n",
    "\tidx  = np.arange(1000)\n",
    "\tnp.random.shuffle(idx)\n",
    "\treturn X[idx], y[idx]\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def add_skip_conn(self, layer1, layer2):\n",
    "        pass\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        output = input_data\n",
    "        for layer in self.layers:\n",
    "          output = layer.forward(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_cost(self, y_true, y_pred):\n",
    "        return np.mean(np.square(y_true-y_pred)) / 2\n",
    "\n",
    "    def compute_cost_grad(self, y_true, y_pred):\n",
    "        return (y_pred - y_true)/y_true.shape[1]\n",
    "\n",
    "    # train the network\n",
    "    def train(self, x_train, y_train, epochs=50, batch_size=2, learning_rate=0.0001):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        \n",
    "        #saving epoch and error in list\n",
    "        loss_list = []\n",
    "\n",
    "        # training loop\n",
    "        for i in tqdm(range(epochs)):\n",
    "            err = 0\n",
    "            # for j in range(samples):\n",
    "            for x, y in batch_iterator(x_train, y_train, batch_size):\n",
    "                # forward propagation\n",
    "                output = x\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                loss_list.append(self.compute_cost(y, output))\n",
    "                err += self.compute_cost(y, output)\n",
    "\n",
    "                # backward propagation\n",
    "                grad = self.compute_cost_grad(y, output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            # print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "        return loss_list\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#load MNIST from server\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#training data\n",
    "#reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0],28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "#ecnoding output\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "#same for test data\n",
    "x_test = x_test.reshape(x_test.shape[0],28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "#ecnoding output\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#Network\n",
    "BATCH_SIZE = 2\n",
    "model = Model()\n",
    "model.add(Dense(28*28, 50, activation=\"sigmoid\"))\n",
    "model.add(Dense(50, 10, activation=\"sigmoid\"))\n",
    "\n",
    "model.train(mse, mse_prime)\n",
    "\n",
    "history = model.fit(x_train[:2000], y_train[:2000],epochs=50,batch_size=BATCH_SIZE,learning_rate=0.1)\n",
    "\n",
    "y_pred = model.predict(x_test[:10])\n",
    "\n",
    "print('true values: ')\n",
    "print(np.argmax(y_test[0:10], axis=1))\n",
    "\n",
    "print('predicted values: ')\n",
    "print(np.argmax(y_pred[0:10], axis=1))\n",
    "\n",
    "\n",
    "\n",
    "class SkipConn(Layer):\n",
    "    def __init__(self, layer1, layer2, input_size, output_size, activation=\"ReLU\"):\n",
    "        Layer.__init__(self,input_size=input_size, output_size=output_size, activation=activation)\n",
    "        self.weight1 = np.random.rand(layer1.get_input_size(), output_size) - 0.5\n",
    "        self.weight2 = np.random.rand(layer2.get_input_size(), output_size) - 0.5\n",
    "        self.bias1 = np.random.rand(1, output_size) - 0.5\n",
    "        self.bias2 = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward(self, A_prev1, A_prev2):\n",
    "        \"\"\"\n",
    "        A_prev1 : Input to the layer that is output of previous layer\n",
    "        A_prev2 : Input to the layer that is skip connection of past layer\n",
    "        \"\"\"\n",
    "        self.linear_cache = (A_prev1, A_prev2)\n",
    "        \n",
    "        self.activation_cache = (np.dot(A_prev1, self.weight1) + self.bias1, np.dot(A_prev2, self.weight2)+self.bias2)\n",
    "\n",
    "        if self.activation == \"ReLU\":\n",
    "            Z = (self._relu(self.activation_cache[0]), self._relu(self.activation[1]))\n",
    "\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            Z = (self._sigmoid(self.activation_cache[0]), self._sigmoid(self.activation[1]))\n",
    "\n",
    "        elif self.activation == \"pseudosigmoid\":\n",
    "            Z = (self._pseudosigmoid(self.activation_cache[0]), self._pseudosigmoid(self.activation[1]))\n",
    "\n",
    "        elif self.activation == \"tanh\":\n",
    "            Z = (self._tanh(self.activation_cache[0]), self._tanh(self.activation[1]))\n",
    "\n",
    "        assert Z[0].shape == Z[1].shape\n",
    "\n",
    "        return Z[0] + Z[1]\n",
    "\n",
    "    def summary(self):\n",
    "      print(f\"\"\"--------------------------------------------------------------\\n\n",
    "          Weight1 : {self.weight1.shape} \\t bias1 : {self.bias1.shape}\\n\n",
    "          Weight2 : {self.weight2.shape} \\t bias2 : {self.bias2.shape}\\n\n",
    "          Input Shape : (None, {self.weight1.shape[0]+self.weight2.shape[0]})\\n\n",
    "          Output Shape : (None, {self.output_size})\"\"\")\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward(self, dA, learning_rate):\n",
    "        dZ = None\n",
    "        if self.activation == \"ReLU\":\n",
    "            dZ = (self._relu_backward(self.activation_cache[0]) * dA, self._relu_backward(self.activation_cache[1]))\n",
    "\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            dZ = (self._sigmoid_backward(self.activation_cache[0]) * dA, self._sigmoid_backward(self.activation_cache[1]))\n",
    "\n",
    "        elif self.activation == \"pseudosigmoid\":\n",
    "            dZ = (self._pseudosigmoid_backward(self.activation_cache[0]) * dA, self._pseudosigmoid_backward(self.activation_cache[1]))\n",
    "        \n",
    "        elif self.activation == \"tanh\":\n",
    "            dZ = (self._tanh_backward(self.activation_cache[0]) * dA, self._tanh_backward(self.activation_cache[1]))\n",
    "        \n",
    "\n",
    "        dW1 = np.dot((self.linear_cache[0]).T, dZ[0])\n",
    "        dW2 = np.dot((self.linear_cache[1]).T, dZ[1])\n",
    "\n",
    "        # dBias = output_error\n",
    "        dB1 = np.sum(dZ[0], axis=0, keepdims=True) / dZ[0].shape[0]\n",
    "        dB2 = np.sum(dZ[1], axis=0, keepdims=True) / dZ[1].shape[0]\n",
    "\n",
    "        dA1 = np.dot(dA[0], self.weight[0].T)        \n",
    "        dA2 = np.dot(dA[1], self.weight[1].T)\n",
    "\n",
    "        # update parameters\n",
    "        self.weight1 -= learning_rate * dW1\n",
    "        self.bias1 -= learning_rate * dB1\n",
    "\n",
    "        self.weight2 -= learning_rate * dW2\n",
    "        self.bias2 -= learning_rate * dB2\n",
    "\n",
    "        return dA1, dA2\n",
    "\n",
    "\n",
    "layers = []\n",
    "\n",
    "layers.append(Dense(28*28, 512, activation=\"sigmoid\"))\n",
    "layers.append(Dense(512, 256, activation=\"sigmoid\"))\n",
    "layers.append(Dense(256, 128, activation=\"sigmoid\"))\n",
    "layers.append(SkipConn(layers[1], layers[2], 0, 64, activation=\"sigmoid\"))\n",
    "layers.append(Dense(64, 10, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "  for x, Y in batch_iterator(x_train, y_train):\n",
    "      # forward propagation\n",
    "      temp_skip = layers[0].forward(X)\n",
    "      temp = layers[1].forward(temp_skip)\n",
    "      temp = layers[2].forward(temp)\n",
    "      temp = layers[3].forward(temp, temp_skip)\n",
    "      temp = layers[4].forward(temp)\n",
    "\n",
    "      # loss calculation\n",
    "      loss = np.mean(np.square(Y-temp)) / 2\n",
    "\n",
    "      # Gradient of loss w.r.t temp\n",
    "      grad = (temp - Y) / Y.shape[1]\n",
    "\n",
    "      grad = layers[4].backward(grad, 0.1)\n",
    "      grad, grad_skip = layers[4].backward(grad, 0.1)\n",
    "      grad = layers[4].backward(grad, 0.1)\n",
    "      grad = layers[4].backward(grad, 0.1)\n",
    "      grad = layers[4].backward(grad+grad_skip, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "for layer in layers:\n",
    "  layer.summary()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
